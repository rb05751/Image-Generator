# Image-Generator

I was inspired to create this model after practicing with various Generative Models over the past month. I wanted to see if it was possible to map output labels to input images rather than the traditional mapping of input images to output labels. My main concern was that even if it is possible in the slightest bit, what would the quality of the generated images be like? The idea of this model is to instead pass a Deconvolutional Network an output label and have it upsample to learn the input image that is associated with that label. For example, with the classic MNIST Handwritten Digits dataset, a model would learn to associate the number 3 with the image of 3, so that if a user passes the model a number (i.e. 1,2,3...10), it would generate an image of the number it was given. It turns out that a model can be built to do this and the quality of the images are fairly accurate in most cases. 

Here are the details of the model:

It is a Deconvolutional Neural Network trained on the MNIST Fashion and Handwritten Digits datasets, that was built using TensorFlow running in Google Colab on their GPU. 
The input layer is a Dense layer that takes, as input, a tensor of shape (batch_size, image label) where the image label is a single digit number, and expands the  number to a 12,544 matrix that is reshaped into a (7,7,256) tensor that is passed to a Deconvolutional layer, which expands it to a (14,14,256) tensor. This is then passed on to the next layer that converts it to a (28,28,1) tensor, which is then flattented into a matrix of (batch_size, 784). The input images (that are now the output labels) were flattened into a vector of 784 pixels to be the ground truth labels to compare the output of the network against. The Dense layer and the first DeConv layer had RELU activations, the final DeConv layer had a Tanh activation and the final layer had a sigmoid activation, to help with learning, since the images were normalized to pixel values between 0 and 1. 

The chosen loss function was Categorical Cross Entropy, note that I tried multiple loss functions, including but not limited to, MSE, MAE, and cosine_similarity. All of which did not perform as well as Categorical Cross Entropy did. 

Findings: The model does not generate 'perfect' images and had issues in the Fashion MNIST dataset with generating the purse image, the heel and the number one in the handwritten digits dataset. Further study will need to occur to test this model on other datasets.
